{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_AJLgPCqYP0",
        "outputId": "bda93970-dc74-476c-a884-9fe14be1ba58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Dataset sizes -> Train: 10796 | Val: 3084 | Test: 1543\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-handwritten and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipython-input-6-3713764460.py:245: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n",
            "/tmp/ipython-input-6-3713764460.py:269: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  step 100/675 | loss 4.2788\n",
            "  step 200/675 | loss 4.1865\n",
            "  step 300/675 | loss 4.1670\n",
            "  step 400/675 | loss 4.0897\n",
            "  step 500/675 | loss 4.1143\n",
            "  step 600/675 | loss 4.0310\n",
            "Epoch 001/50 | TLoss 4.1636 Acc  5.24% | VLoss 3.9803 Acc 10.96%\n",
            "  step 100/675 | loss 3.9023\n",
            "  step 200/675 | loss 3.8717\n",
            "  step 300/675 | loss 3.8628\n",
            "  step 400/675 | loss 3.9113\n",
            "  step 500/675 | loss 3.9352\n",
            "  step 600/675 | loss 3.7052\n",
            "Epoch 002/50 | TLoss 3.8524 Acc 16.98% | VLoss 3.6889 Acc 24.03%\n",
            "  step 100/675 | loss 3.6724\n",
            "  step 200/675 | loss 3.4314\n",
            "  step 300/675 | loss 3.7306\n",
            "  step 400/675 | loss 3.6039\n",
            "  step 500/675 | loss 3.5320\n",
            "  step 600/675 | loss 3.5645\n",
            "Epoch 003/50 | TLoss 3.6004 Acc 27.51% | VLoss 3.4531 Acc 33.14%\n",
            "  step 100/675 | loss 3.3826\n",
            "  step 200/675 | loss 3.3110\n",
            "  step 300/675 | loss 3.6094\n",
            "  step 400/675 | loss 3.4567\n",
            "  step 500/675 | loss 3.5571\n",
            "  step 600/675 | loss 3.2726\n",
            "Epoch 004/50 | TLoss 3.3933 Acc 34.09% | VLoss 3.2599 Acc 35.51%\n",
            "  step 100/675 | loss 3.3281\n",
            "  step 200/675 | loss 3.1870\n",
            "  step 300/675 | loss 3.1053\n",
            "  step 400/675 | loss 2.9756\n",
            "  step 500/675 | loss 2.9037\n",
            "  step 600/675 | loss 3.1548\n",
            "Epoch 005/50 | TLoss 3.2177 Acc 37.72% | VLoss 3.0888 Acc 42.74%\n",
            "Unfreezing encoder for fine-tuning...\n",
            "  step 100/675 | loss 1.5081\n",
            "  step 200/675 | loss 1.2364\n",
            "  step 300/675 | loss 0.9404\n",
            "  step 400/675 | loss 0.6921\n",
            "  step 500/675 | loss 1.2058\n",
            "  step 600/675 | loss 0.5747\n",
            "Epoch 006/50 | TLoss 1.0053 Acc 72.39% | VLoss 0.5484 Acc 83.79%\n",
            "  step 100/675 | loss 0.3816\n",
            "  step 200/675 | loss 0.6845\n",
            "  step 300/675 | loss 0.6453\n",
            "  step 400/675 | loss 0.2693\n",
            "  step 500/675 | loss 0.2471\n",
            "  step 600/675 | loss 0.3099\n",
            "Epoch 007/50 | TLoss 0.4015 Acc 88.32% | VLoss 0.3415 Acc 88.98%\n",
            "  step 100/675 | loss 0.2002\n",
            "  step 200/675 | loss 0.1901\n",
            "  step 300/675 | loss 0.2620\n",
            "  step 400/675 | loss 0.0643\n",
            "  step 500/675 | loss 0.1615\n",
            "  step 600/675 | loss 0.1772\n",
            "Epoch 008/50 | TLoss 0.2602 Acc 92.03% | VLoss 0.2478 Acc 91.93%\n",
            "  step 100/675 | loss 0.2951\n",
            "  step 200/675 | loss 0.1383\n",
            "  step 300/675 | loss 0.6692\n",
            "  step 400/675 | loss 0.5187\n",
            "  step 500/675 | loss 0.0687\n",
            "  step 600/675 | loss 0.0516\n",
            "Epoch 009/50 | TLoss 0.1956 Acc 93.89% | VLoss 0.2114 Acc 92.93%\n",
            "  step 100/675 | loss 0.2350\n",
            "  step 200/675 | loss 0.1977\n",
            "  step 300/675 | loss 0.0617\n",
            "  step 400/675 | loss 0.1184\n",
            "  step 500/675 | loss 0.3113\n",
            "  step 600/675 | loss 0.2517\n",
            "Epoch 010/50 | TLoss 0.1480 Acc 95.07% | VLoss 0.1958 Acc 93.55%\n",
            "  step 100/675 | loss 0.0696\n",
            "  step 200/675 | loss 0.0513\n",
            "  step 300/675 | loss 0.3677\n",
            "  step 400/675 | loss 0.2958\n",
            "  step 500/675 | loss 0.1417\n",
            "  step 600/675 | loss 0.1845\n",
            "Epoch 011/50 | TLoss 0.1227 Acc 95.78% | VLoss 0.2265 Acc 92.61%\n",
            "  step 100/675 | loss 0.0171\n",
            "  step 200/675 | loss 0.1919\n",
            "  step 300/675 | loss 0.0095\n",
            "  step 400/675 | loss 0.0613\n",
            "  step 500/675 | loss 0.0190\n",
            "  step 600/675 | loss 0.1161\n",
            "Epoch 012/50 | TLoss 0.1090 Acc 96.28% | VLoss 0.1859 Acc 93.84%\n",
            "  step 100/675 | loss 0.1938\n",
            "  step 200/675 | loss 0.1131\n",
            "  step 300/675 | loss 0.0234\n",
            "  step 400/675 | loss 0.0808\n",
            "  step 500/675 | loss 0.5273\n",
            "  step 600/675 | loss 0.0732\n",
            "Epoch 013/50 | TLoss 0.0871 Acc 96.75% | VLoss 0.1459 Acc 94.84%\n",
            "  step 100/675 | loss 0.0708\n",
            "  step 200/675 | loss 0.0455\n",
            "  step 300/675 | loss 0.0029\n",
            "  step 400/675 | loss 0.1000\n",
            "  step 500/675 | loss 0.0028\n",
            "  step 600/675 | loss 0.0118\n",
            "Epoch 014/50 | TLoss 0.0770 Acc 97.30% | VLoss 0.1477 Acc 95.20%\n",
            "  step 100/675 | loss 0.1761\n",
            "  step 200/675 | loss 0.0166\n",
            "  step 300/675 | loss 0.0130\n",
            "  step 400/675 | loss 0.0483\n",
            "  step 500/675 | loss 0.0441\n",
            "  step 600/675 | loss 0.0048\n",
            "Epoch 015/50 | TLoss 0.0712 Acc 97.11% | VLoss 0.1418 Acc 95.20%\n",
            "  step 100/675 | loss 0.0041\n",
            "  step 200/675 | loss 0.0124\n",
            "  step 300/675 | loss 0.0157\n",
            "  step 400/675 | loss 0.0063\n",
            "  step 500/675 | loss 0.4850\n",
            "  step 600/675 | loss 0.0241\n",
            "Epoch 016/50 | TLoss 0.0600 Acc 97.75% | VLoss 0.1482 Acc 94.88%\n",
            "  step 100/675 | loss 0.0897\n",
            "  step 200/675 | loss 0.0712\n",
            "  step 300/675 | loss 0.0199\n",
            "  step 400/675 | loss 0.1257\n",
            "  step 500/675 | loss 0.0131\n",
            "  step 600/675 | loss 0.0325\n",
            "Epoch 017/50 | TLoss 0.0643 Acc 97.56% | VLoss 0.1730 Acc 94.81%\n",
            "  step 100/675 | loss 0.0120\n",
            "  step 200/675 | loss 0.0035\n",
            "  step 300/675 | loss 0.0055\n",
            "  step 400/675 | loss 0.0856\n",
            "  step 500/675 | loss 0.0048\n",
            "  step 600/675 | loss 0.0014\n",
            "Epoch 018/50 | TLoss 0.0355 Acc 98.45% | VLoss 0.1060 Acc 96.53%\n",
            "  step 100/675 | loss 0.0049\n",
            "  step 200/675 | loss 0.0013\n",
            "  step 300/675 | loss 0.0031\n",
            "  step 400/675 | loss 0.0097\n",
            "  step 500/675 | loss 0.0038\n",
            "  step 600/675 | loss 0.0394\n",
            "Epoch 019/50 | TLoss 0.0287 Acc 98.76% | VLoss 0.1137 Acc 96.34%\n",
            "  step 100/675 | loss 0.0072\n",
            "  step 200/675 | loss 0.0015\n",
            "  step 300/675 | loss 0.0025\n",
            "  step 400/675 | loss 0.0012\n",
            "  step 500/675 | loss 0.0245\n",
            "  step 600/675 | loss 0.0045\n",
            "Epoch 020/50 | TLoss 0.0308 Acc 98.61% | VLoss 0.1078 Acc 96.53%\n",
            "  step 100/675 | loss 0.0056\n",
            "  step 200/675 | loss 0.0383\n",
            "  step 300/675 | loss 0.0362\n",
            "  step 400/675 | loss 0.0051\n",
            "  step 500/675 | loss 0.0591\n",
            "  step 600/675 | loss 0.0589\n",
            "Epoch 021/50 | TLoss 0.0317 Acc 98.51% | VLoss 0.1028 Acc 96.89%\n",
            "  step 100/675 | loss 0.0267\n",
            "  step 200/675 | loss 0.0604\n",
            "  step 300/675 | loss 0.0399\n",
            "  step 400/675 | loss 0.0479\n",
            "  step 500/675 | loss 0.0034\n",
            "  step 600/675 | loss 0.0016\n",
            "Epoch 022/50 | TLoss 0.0295 Acc 98.64% | VLoss 0.1177 Acc 96.21%\n",
            "  step 100/675 | loss 0.0323\n",
            "  step 200/675 | loss 0.0756\n",
            "  step 300/675 | loss 0.0141\n",
            "  step 400/675 | loss 0.0009\n",
            "  step 500/675 | loss 0.0579\n",
            "  step 600/675 | loss 0.0475\n",
            "Epoch 023/50 | TLoss 0.0326 Acc 98.62% | VLoss 0.1021 Acc 96.76%\n",
            "  step 100/675 | loss 0.0076\n",
            "  step 200/675 | loss 0.0039\n",
            "  step 300/675 | loss 0.0004\n",
            "  step 400/675 | loss 0.0038\n",
            "  step 500/675 | loss 0.0010\n",
            "  step 600/675 | loss 0.0468\n",
            "Epoch 024/50 | TLoss 0.0265 Acc 98.70% | VLoss 0.1162 Acc 96.37%\n",
            "  step 100/675 | loss 0.0374\n",
            "  step 200/675 | loss 0.0753\n",
            "  step 300/675 | loss 0.0012\n",
            "  step 400/675 | loss 0.0013\n",
            "  step 500/675 | loss 0.0035\n",
            "  step 600/675 | loss 0.0642\n",
            "Epoch 025/50 | TLoss 0.0222 Acc 98.91% | VLoss 0.1147 Acc 96.56%\n",
            "  step 100/675 | loss 0.0459\n",
            "  step 200/675 | loss 0.1213\n",
            "  step 300/675 | loss 0.0026\n",
            "  step 400/675 | loss 0.0060\n",
            "  step 500/675 | loss 0.1095\n",
            "  step 600/675 | loss 0.0010\n",
            "Epoch 026/50 | TLoss 0.0215 Acc 98.97% | VLoss 0.0992 Acc 96.82%\n",
            "Early stopping triggered!\n",
            "Best validation accuracy: 96.89%\n",
            "\n",
            "Test Set Evaluation:\n",
            "Accuracy       : 96.18%\n",
            "Precision (avg): 0.9614\n",
            "Recall    (avg): 0.9634\n",
            "F1 Score  (avg): 0.9616\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-6-3713764460.py:410: UserWarning: Glyph 2990 (\\N{TAMIL LETTER MA}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "/tmp/ipython-input-6-3713764460.py:410: UserWarning: Matplotlib currently does not support Tamil natively.\n",
            "  plt.tight_layout()\n",
            "/tmp/ipython-input-6-3713764460.py:410: UserWarning: Glyph 3010 (\\N{TAMIL VOWEL SIGN UU}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "/tmp/ipython-input-6-3713764460.py:410: UserWarning: Glyph 2991 (\\N{TAMIL LETTER YA}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "/tmp/ipython-input-6-3713764460.py:410: UserWarning: Glyph 2993 (\\N{TAMIL LETTER RRA}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "/tmp/ipython-input-6-3713764460.py:410: UserWarning: Glyph 2979 (\\N{TAMIL LETTER NNA}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "/tmp/ipython-input-6-3713764460.py:410: UserWarning: Glyph 2969 (\\N{TAMIL LETTER NGA}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "/tmp/ipython-input-6-3713764460.py:410: UserWarning: Glyph 2997 (\\N{TAMIL LETTER VA}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "/tmp/ipython-input-6-3713764460.py:410: UserWarning: Glyph 3007 (\\N{TAMIL VOWEL SIGN I}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "/tmp/ipython-input-6-3713764460.py:410: UserWarning: Glyph 2985 (\\N{TAMIL LETTER NNNA}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "/tmp/ipython-input-6-3713764460.py:410: UserWarning: Glyph 2986 (\\N{TAMIL LETTER PA}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "/tmp/ipython-input-6-3713764460.py:410: UserWarning: Glyph 2975 (\\N{TAMIL LETTER TTA}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "/tmp/ipython-input-6-3713764460.py:410: UserWarning: Glyph 3009 (\\N{TAMIL VOWEL SIGN U}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "/tmp/ipython-input-6-3713764460.py:410: UserWarning: Glyph 2994 (\\N{TAMIL LETTER LA}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "/tmp/ipython-input-6-3713764460.py:410: UserWarning: Glyph 2980 (\\N{TAMIL LETTER TA}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "/tmp/ipython-input-6-3713764460.py:410: UserWarning: Glyph 2996 (\\N{TAMIL LETTER LLLA}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "/tmp/ipython-input-6-3713764460.py:410: UserWarning: Glyph 2958 (\\N{TAMIL LETTER E}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "/tmp/ipython-input-6-3713764460.py:410: UserWarning: Glyph 2984 (\\N{TAMIL LETTER NA}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "/tmp/ipython-input-6-3713764460.py:410: UserWarning: Glyph 2965 (\\N{TAMIL LETTER KA}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "/tmp/ipython-input-6-3713764460.py:410: UserWarning: Glyph 3008 (\\N{TAMIL VOWEL SIGN II}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "/tmp/ipython-input-6-3713764460.py:410: UserWarning: Glyph 2949 (\\N{TAMIL LETTER A}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "/tmp/ipython-input-6-3713764460.py:410: UserWarning: Glyph 2970 (\\N{TAMIL LETTER CA}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "/tmp/ipython-input-6-3713764460.py:410: UserWarning: Glyph 2995 (\\N{TAMIL LETTER LLA}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "/tmp/ipython-input-6-3713764460.py:410: UserWarning: Glyph 2992 (\\N{TAMIL LETTER RA}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "/tmp/ipython-input-6-3713764460.py:411: UserWarning: Glyph 2990 (\\N{TAMIL LETTER MA}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(os.path.join(OUTPUT_DIR, \"confusion_matrix.png\"), dpi=150)\n",
            "/tmp/ipython-input-6-3713764460.py:411: UserWarning: Matplotlib currently does not support Tamil natively.\n",
            "  plt.savefig(os.path.join(OUTPUT_DIR, \"confusion_matrix.png\"), dpi=150)\n",
            "/tmp/ipython-input-6-3713764460.py:411: UserWarning: Glyph 3010 (\\N{TAMIL VOWEL SIGN UU}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(os.path.join(OUTPUT_DIR, \"confusion_matrix.png\"), dpi=150)\n",
            "/tmp/ipython-input-6-3713764460.py:411: UserWarning: Glyph 2991 (\\N{TAMIL LETTER YA}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(os.path.join(OUTPUT_DIR, \"confusion_matrix.png\"), dpi=150)\n",
            "/tmp/ipython-input-6-3713764460.py:411: UserWarning: Glyph 2993 (\\N{TAMIL LETTER RRA}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(os.path.join(OUTPUT_DIR, \"confusion_matrix.png\"), dpi=150)\n",
            "/tmp/ipython-input-6-3713764460.py:411: UserWarning: Glyph 2979 (\\N{TAMIL LETTER NNA}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(os.path.join(OUTPUT_DIR, \"confusion_matrix.png\"), dpi=150)\n",
            "/tmp/ipython-input-6-3713764460.py:411: UserWarning: Glyph 2969 (\\N{TAMIL LETTER NGA}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(os.path.join(OUTPUT_DIR, \"confusion_matrix.png\"), dpi=150)\n",
            "/tmp/ipython-input-6-3713764460.py:411: UserWarning: Glyph 2997 (\\N{TAMIL LETTER VA}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(os.path.join(OUTPUT_DIR, \"confusion_matrix.png\"), dpi=150)\n",
            "/tmp/ipython-input-6-3713764460.py:411: UserWarning: Glyph 3007 (\\N{TAMIL VOWEL SIGN I}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(os.path.join(OUTPUT_DIR, \"confusion_matrix.png\"), dpi=150)\n",
            "/tmp/ipython-input-6-3713764460.py:411: UserWarning: Glyph 2985 (\\N{TAMIL LETTER NNNA}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(os.path.join(OUTPUT_DIR, \"confusion_matrix.png\"), dpi=150)\n",
            "/tmp/ipython-input-6-3713764460.py:411: UserWarning: Glyph 2986 (\\N{TAMIL LETTER PA}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(os.path.join(OUTPUT_DIR, \"confusion_matrix.png\"), dpi=150)\n",
            "/tmp/ipython-input-6-3713764460.py:411: UserWarning: Glyph 2975 (\\N{TAMIL LETTER TTA}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(os.path.join(OUTPUT_DIR, \"confusion_matrix.png\"), dpi=150)\n",
            "/tmp/ipython-input-6-3713764460.py:411: UserWarning: Glyph 3009 (\\N{TAMIL VOWEL SIGN U}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(os.path.join(OUTPUT_DIR, \"confusion_matrix.png\"), dpi=150)\n",
            "/tmp/ipython-input-6-3713764460.py:411: UserWarning: Glyph 2994 (\\N{TAMIL LETTER LA}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(os.path.join(OUTPUT_DIR, \"confusion_matrix.png\"), dpi=150)\n",
            "/tmp/ipython-input-6-3713764460.py:411: UserWarning: Glyph 2980 (\\N{TAMIL LETTER TA}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(os.path.join(OUTPUT_DIR, \"confusion_matrix.png\"), dpi=150)\n",
            "/tmp/ipython-input-6-3713764460.py:411: UserWarning: Glyph 2996 (\\N{TAMIL LETTER LLLA}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(os.path.join(OUTPUT_DIR, \"confusion_matrix.png\"), dpi=150)\n",
            "/tmp/ipython-input-6-3713764460.py:411: UserWarning: Glyph 2958 (\\N{TAMIL LETTER E}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(os.path.join(OUTPUT_DIR, \"confusion_matrix.png\"), dpi=150)\n",
            "/tmp/ipython-input-6-3713764460.py:411: UserWarning: Glyph 2984 (\\N{TAMIL LETTER NA}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(os.path.join(OUTPUT_DIR, \"confusion_matrix.png\"), dpi=150)\n",
            "/tmp/ipython-input-6-3713764460.py:411: UserWarning: Glyph 2965 (\\N{TAMIL LETTER KA}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(os.path.join(OUTPUT_DIR, \"confusion_matrix.png\"), dpi=150)\n",
            "/tmp/ipython-input-6-3713764460.py:411: UserWarning: Glyph 3008 (\\N{TAMIL VOWEL SIGN II}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(os.path.join(OUTPUT_DIR, \"confusion_matrix.png\"), dpi=150)\n",
            "/tmp/ipython-input-6-3713764460.py:411: UserWarning: Glyph 2949 (\\N{TAMIL LETTER A}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(os.path.join(OUTPUT_DIR, \"confusion_matrix.png\"), dpi=150)\n",
            "/tmp/ipython-input-6-3713764460.py:411: UserWarning: Glyph 2970 (\\N{TAMIL LETTER CA}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(os.path.join(OUTPUT_DIR, \"confusion_matrix.png\"), dpi=150)\n",
            "/tmp/ipython-input-6-3713764460.py:411: UserWarning: Glyph 2995 (\\N{TAMIL LETTER LLA}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(os.path.join(OUTPUT_DIR, \"confusion_matrix.png\"), dpi=150)\n",
            "/tmp/ipython-input-6-3713764460.py:411: UserWarning: Glyph 2992 (\\N{TAMIL LETTER RA}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(os.path.join(OUTPUT_DIR, \"confusion_matrix.png\"), dpi=150)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Done. Outputs saved in: /content/trocr_glyph_out\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "#!/usr/bin/env python\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "TrOCR-based Glyph Classifier (Optimized for Faster Training on T4 GPU)\n",
        "=====================================================================\n",
        "\n",
        "Features:\n",
        "- Uses **microsoft/trocr-base-handwritten** encoder for glyph classification (75 classes).\n",
        "- **Train / Validation / Test split = 70 / 20 / 10**.\n",
        "- **Data augmentation** (flip, rotation, color jitter).\n",
        "- **Configurable image size** (default 256 for speed; can set 384 to match pretraining).\n",
        "- **Mixed Precision (AMP)** training for speed & reduced memory.\n",
        "- **Gradient Accumulation** option to simulate larger batch sizes.\n",
        "- **Encoder Freeze / Unfreeze** strategy: freeze for first N epochs, then fine-tune.\n",
        "- **Dual Learning Rates:** low LR for encoder, higher LR for classifier head.\n",
        "- **Early Stopping** on validation accuracy (patience configurable).\n",
        "- **Learning Rate Scheduler (ReduceLROnPlateau)** triggered by validation accuracy plateau.\n",
        "- **Saves:** best_model.pth (best val acc), final_model.pth (last/early-stop), checkpoints, metrics, plots, confusion matrix, classification report.\n",
        "\n",
        "This script is designed to **reduce training time** on a T4 GPU while retaining the TrOCR backbone.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import json\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from typing import Tuple, Dict, Any\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        ")\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision import transforms\n",
        "\n",
        "from transformers import VisionEncoderDecoderModel, TrOCRProcessor\n",
        "\n",
        "# =============================================================\n",
        "# 0. Configuration\n",
        "# =============================================================\n",
        "DATA_DIR = \"/content/data/Final DATASET\"  # <<< update if needed\n",
        "OUTPUT_DIR = \"/content/trocr_glyph_out\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# --- Speed / Memory Controls --------------------------------------------------\n",
        "IMG_SIZE = (384,384)   # smaller than 384 speeds training; set (384,384) to match TrOCR pretraining\n",
        "BATCH_SIZE = 16         # reduce if OOM; increase if GPU memory allows\n",
        "ACCUM_STEPS = 1         # >1 simulates larger batch; increases wall clock but improves stability\n",
        "NUM_WORKERS = 2         # dataloader workers; adjust for Colab\n",
        "PIN_MEMORY = True\n",
        "\n",
        "# --- Training Hyperparams -----------------------------------------------------\n",
        "EPOCHS = 50\n",
        "FREEZE_EPOCHS = 5       # freeze encoder for first N epochs (fast classifier warmup)\n",
        "LR_ENCODER = 1e-5       # low LR for pretrained encoder\n",
        "LR_CLASSIFIER = 1e-4    # higher LR for classifier head\n",
        "PATIENCE = 5            # early stopping patience (epochs w/out val_acc improvement)\n",
        "LR_SCHED_FACTOR = 0.5   # ReduceLROnPlateau factor\n",
        "LR_SCHED_PATIENCE = 2   # plateau patience before LR drop\n",
        "MIN_LR = 1e-6\n",
        "PRINT_EVERY = 100       # steps\n",
        "SEED = 42\n",
        "\n",
        "# --- Model -----------------------------------------------------\n",
        "BASE_MODEL = \"microsoft/trocr-base-handwritten\"\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# =============================================================\n",
        "# 1. Label Map\n",
        "# =============================================================\n",
        "label_map = {\n",
        "    0: 'மூ', 1: 'ா', 2: 'ட', 3: 'ம', 4: 'ப', 5: 'ெ', 6: 'ய', 7: 'ல ', 8: 'ன',\n",
        "    9: 'ற', 10: 'க', 11: 'வ', 12: 'ண', 13: 'த', 14: 'ச', 15: 'ங', 16: 'ள',\n",
        "    17: 'தி', 18: 'வி', 19: 'றி', 20: 'லி', 21: 'னி', 22: 'யி', 23: 'ழி',\n",
        "    24: 'பி', 25: 'ரி', 26: 'சி', 27: 'ணி', 28: 'மி', 29: 'கி', 30: 'டு',\n",
        "    31: 'கு', 32: 'ளு', 33: 'லு', 34: 'மு', 35: 'ணு', 36: 'னு', 37: 'ஞ',\n",
        "    38: 'பு', 39: 'று', 40: 'ரு', 41: 'சு', 42: 'து', 43: 'வு', 44: 'யு',\n",
        "    45: 'ழு', 46: 'டி', 47: 'ளி', 48: 'எ', 49: 'ழ', 50: 'கீ', 51: 'றூ',\n",
        "    52: 'மீ', 53: 'வீ', 54: 'நூ', 55: 'றீ', 56: 'தீ', 57: 'கூ', 58: 'தூ',\n",
        "    59: 'சூ', 60: 'யீ', 61: 'லூ', 62: 'உ', 63: 'அ', 64: 'ழீ', 65: 'யூ',\n",
        "    66: 'சீ', 67: 'ணீ', 68: 'ஆ', 69: 'ளு', 70: 'இ', 71: 'ை', 72: 'ர', 73: 'ந', 74: 'ஒ'\n",
        "}\n",
        "NUM_CLASSES = len(label_map)\n",
        "\n",
        "with open(os.path.join(OUTPUT_DIR, \"id2label.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(label_map, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "# =============================================================\n",
        "# 2. Reproducibility\n",
        "# =============================================================\n",
        "def set_seed(seed=SEED):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = False  # True slows but deterministic\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "set_seed()\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "# =============================================================\n",
        "# 3. Processor & Normalization Stats\n",
        "# =============================================================\n",
        "# We use the TrOCRProcessor only to pull image mean/std so that our manual transforms\n",
        "# match the normalization the encoder expects. This avoids per-batch processor overhead.\n",
        "processor = TrOCRProcessor.from_pretrained(BASE_MODEL)\n",
        "image_mean = processor.image_processor.image_mean\n",
        "image_std = processor.image_processor.image_std\n",
        "\n",
        "# torchvision expects 3-channel mean/std; ensure list of len 3\n",
        "if len(image_mean) == 1:  # fallback safety\n",
        "    image_mean = image_mean * 3\n",
        "    image_std = image_std * 3\n",
        "\n",
        "# =============================================================\n",
        "# 4. Dataset\n",
        "# =============================================================\n",
        "class GlyphDataset(Dataset):\n",
        "    def __init__(self, data_dir: str, transform=None):\n",
        "        self.samples = []\n",
        "        self.transform = transform\n",
        "        for label in range(NUM_CLASSES):\n",
        "            folder = os.path.join(data_dir, str(label))\n",
        "            if not os.path.exists(folder):\n",
        "                continue\n",
        "            for file in os.listdir(folder):\n",
        "                if file.lower().endswith((\".png\", \".jpg\", \".jpeg\", \".bmp\", \".tif\", \".tiff\")):\n",
        "                    self.samples.append((os.path.join(folder, file), label))\n",
        "        random.shuffle(self.samples)\n",
        "        if len(self.samples) == 0:\n",
        "            raise RuntimeError(f\"No images found in {data_dir}.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path, label = self.samples[idx]\n",
        "        img = Image.open(path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, label\n",
        "\n",
        "# Data augmentation pipeline ---------------------------------------------------\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize(IMG_SIZE),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=image_mean, std=image_std),\n",
        "])\n",
        "\n",
        "# Validation / Test: no augmentation ------------------------------------------\n",
        "eval_transform = transforms.Compose([\n",
        "    transforms.Resize(IMG_SIZE),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=image_mean, std=image_std),\n",
        "])\n",
        "\n",
        "dataset_full = GlyphDataset(DATA_DIR, transform=None)  # base dataset reads paths only\n",
        "\n",
        "# We'll apply different transforms per split by wrapping Subset -> custom dataset ----------\n",
        "class SubsetWithTransform(Dataset):\n",
        "    def __init__(self, subset, transform):\n",
        "        self.subset = subset\n",
        "        self.indices = subset.indices  # torch.utils.data.Subset\n",
        "        self.dataset = subset.dataset  # original GlyphDataset (no transform)\n",
        "        self.transform = transform\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "    def __getitem__(self, i):\n",
        "        idx = self.indices[i]\n",
        "        path, label = self.dataset.samples[idx]\n",
        "        img = Image.open(path).convert(\"RGB\")\n",
        "        img = self.transform(img)\n",
        "        return img, label\n",
        "\n",
        "n_total = len(dataset_full)\n",
        "n_train = int(0.7 * n_total)\n",
        "n_val = int(0.2 * n_total)\n",
        "n_test = n_total - n_train - n_val\n",
        "\n",
        "train_subset, val_subset, test_subset = random_split(dataset_full, [n_train, n_val, n_test])\n",
        "\n",
        "train_data = SubsetWithTransform(train_subset, train_transform)\n",
        "val_data = SubsetWithTransform(val_subset, eval_transform)\n",
        "test_data = SubsetWithTransform(test_subset, eval_transform)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
        "val_loader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
        "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
        "\n",
        "print(f\"Dataset sizes -> Train: {len(train_data)} | Val: {len(val_data)} | Test: {len(test_data)}\")\n",
        "\n",
        "# =============================================================\n",
        "# 5. Model Definition (TrOCR Encoder + Classifier Head)\n",
        "# =============================================================\n",
        "class TrOCRClassifier(nn.Module):\n",
        "    def __init__(self, base_model_name: str, num_classes: int):\n",
        "        super().__init__()\n",
        "        base_model = VisionEncoderDecoderModel.from_pretrained(base_model_name)\n",
        "        self.encoder = base_model.encoder  # ViT encoder\n",
        "        hidden = self.encoder.config.hidden_size\n",
        "        self.classifier = nn.Linear(hidden, num_classes)\n",
        "\n",
        "    def forward(self, pixel_values):\n",
        "        # pixel_values: [B,3,H,W]\n",
        "        enc_out = self.encoder(pixel_values=pixel_values)\n",
        "        # enc_out.last_hidden_state: [B, seq_len, hidden]\n",
        "        pooled = enc_out.last_hidden_state.mean(dim=1)  # mean pool tokens\n",
        "        logits = self.classifier(pooled)\n",
        "        return logits\n",
        "\n",
        "model = TrOCRClassifier(BASE_MODEL, NUM_CLASSES).to(DEVICE)\n",
        "\n",
        "# Freeze encoder initially -----------------------------------------------------\n",
        "for p in model.encoder.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "# Optimizer with parameter groups ----------------------------------------------\n",
        "optimizer = torch.optim.Adam([\n",
        "    {\"params\": [p for p in model.encoder.parameters() if p.requires_grad], \"lr\": LR_ENCODER},\n",
        "    {\"params\": model.classifier.parameters(), \"lr\": LR_CLASSIFIER},\n",
        "])\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# LR scheduler (Reduce on plateau of val_acc) ----------------------------------\n",
        "# We'll step scheduler manually after each epoch with validation accuracy.\n",
        "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, mode=\"max\", factor=LR_SCHED_FACTOR, patience=LR_SCHED_PATIENCE, min_lr=MIN_LR, verbose=True\n",
        ")\n",
        "\n",
        "# AMP scaler -------------------------------------------------------------------\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n",
        "\n",
        "# =============================================================\n",
        "# 6. Training Loop with Early Stopping, Freeze/Unfreeze, AMP, Accumulation\n",
        "# =============================================================\n",
        "\n",
        "def run_epoch(loader, train_mode: bool, epoch: int) -> Tuple[float, float]:\n",
        "    if train_mode:\n",
        "        model.train()\n",
        "    else:\n",
        "        model.eval()\n",
        "\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    if train_mode:\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    for step, (imgs, labels) in enumerate(loader):\n",
        "        imgs = imgs.to(DEVICE, non_blocking=True)\n",
        "        labels = labels.to(DEVICE, non_blocking=True)\n",
        "\n",
        "        with torch.set_grad_enabled(train_mode):\n",
        "            with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
        "                logits = model(imgs)\n",
        "                loss = criterion(logits, labels)\n",
        "\n",
        "            if train_mode:\n",
        "                scaler.scale(loss / ACCUM_STEPS).backward()\n",
        "                if (step + 1) % ACCUM_STEPS == 0:\n",
        "                    scaler.step(optimizer)\n",
        "                    scaler.update()\n",
        "                    optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        # metrics\n",
        "        total_loss += loss.item() * imgs.size(0)\n",
        "        preds = logits.argmax(dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "        if train_mode and (step + 1) % PRINT_EVERY == 0:\n",
        "            print(f\"  step {step+1}/{len(loader)} | loss {loss.item():.4f}\")\n",
        "\n",
        "    avg_loss = total_loss / max(total, 1)\n",
        "    acc = correct / max(total, 1)\n",
        "    return avg_loss, acc\n",
        "\n",
        "\n",
        "def train_model():\n",
        "    best_val_acc = 0.0\n",
        "    patience_counter = 0\n",
        "    history = {\"epoch\": [], \"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        # Unfreeze encoder at start of fine-tuning phase -----------------------\n",
        "        if epoch == FREEZE_EPOCHS:\n",
        "            print(\"Unfreezing encoder for fine-tuning...\")\n",
        "            for p in model.encoder.parameters():\n",
        "                p.requires_grad = True\n",
        "            # Rebuild optimizer to include encoder params w/ proper LR ----------\n",
        "            optimizer.param_groups.clear()\n",
        "            optimizer.add_param_group({\"params\": model.encoder.parameters(), \"lr\": LR_ENCODER})\n",
        "            optimizer.add_param_group({\"params\": model.classifier.parameters(), \"lr\": LR_CLASSIFIER})\n",
        "\n",
        "        train_loss, train_acc = run_epoch(train_loader, train_mode=True, epoch=epoch)\n",
        "        val_loss, val_acc = run_epoch(val_loader, train_mode=False, epoch=epoch)\n",
        "\n",
        "        history[\"epoch\"].append(epoch + 1)\n",
        "        history[\"train_loss\"].append(train_loss)\n",
        "        history[\"train_acc\"].append(train_acc)\n",
        "        history[\"val_loss\"].append(val_loss)\n",
        "        history[\"val_acc\"].append(val_acc)\n",
        "\n",
        "        print(f\"Epoch {epoch+1:03d}/{EPOCHS} | TLoss {train_loss:.4f} Acc {train_acc*100:5.2f}% | VLoss {val_loss:.4f} Acc {val_acc*100:5.2f}%\")\n",
        "\n",
        "        # LR Scheduler step ---------------------------------------------------\n",
        "        lr_scheduler.step(val_acc)\n",
        "\n",
        "        # Early stopping ------------------------------------------------------\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            patience_counter = 0\n",
        "            torch.save(model.state_dict(), os.path.join(OUTPUT_DIR, \"best_model.pth\"))\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= PATIENCE:\n",
        "                print(\"Early stopping triggered!\")\n",
        "                break\n",
        "\n",
        "    # Always save final model (may be early-stopped)\n",
        "    torch.save(model.state_dict(), os.path.join(OUTPUT_DIR, \"final_model.pth\"))\n",
        "\n",
        "    # Save history ------------------------------------------------------------\n",
        "    import pandas as pd\n",
        "    hist_path = os.path.join(OUTPUT_DIR, \"training_history.csv\")\n",
        "    pd.DataFrame(history).to_csv(hist_path, index=False)\n",
        "\n",
        "    return history, best_val_acc\n",
        "\n",
        "\n",
        "history, best_val_acc = train_model()\n",
        "print(f\"Best validation accuracy: {best_val_acc*100:.2f}%\")\n",
        "\n",
        "# =============================================================\n",
        "# 7. Test Evaluation\n",
        "# =============================================================\n",
        "model.load_state_dict(torch.load(os.path.join(OUTPUT_DIR, \"best_model.pth\"), map_location=DEVICE))\n",
        "model.to(DEVICE)\n",
        "model.eval()\n",
        "\n",
        "all_labels = []\n",
        "all_preds = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in test_loader:\n",
        "        imgs = imgs.to(DEVICE, non_blocking=True)\n",
        "        logits = model(imgs)\n",
        "        preds = logits.argmax(dim=1).cpu().numpy()\n",
        "        all_preds.extend(preds)\n",
        "        all_labels.extend(labels.numpy())\n",
        "\n",
        "# Metrics ---------------------------------------------------------------------\n",
        "acc = accuracy_score(all_labels, all_preds)\n",
        "prec = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
        "rec = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n",
        "f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
        "\n",
        "print(\"\\nTest Set Evaluation:\")\n",
        "print(f\"Accuracy       : {acc*100:.2f}%\")\n",
        "print(f\"Precision (avg): {prec:.4f}\")\n",
        "print(f\"Recall    (avg): {rec:.4f}\")\n",
        "print(f\"F1 Score  (avg): {f1:.4f}\")\n",
        "\n",
        "# Classification report -------------------------------------------------------\n",
        "report = classification_report(\n",
        "    all_labels,\n",
        "    all_preds,\n",
        "    target_names=[label_map[i] for i in range(NUM_CLASSES)],\n",
        "    zero_division=0,\n",
        ")\n",
        "\n",
        "with open(os.path.join(OUTPUT_DIR, \"classification_report.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(report)\n",
        "\n",
        "# Also save CSV version -------------------------------------------------------\n",
        "import pandas as pd\n",
        "rep_dict = classification_report(\n",
        "    all_labels,\n",
        "    all_preds,\n",
        "    target_names=[label_map[i] for i in range(NUM_CLASSES)],\n",
        "    output_dict=True,\n",
        "    zero_division=0,\n",
        ")\n",
        "pd.DataFrame(rep_dict).transpose().to_csv(os.path.join(OUTPUT_DIR, \"classification_report.csv\"), index=True)\n",
        "\n",
        "# Confusion Matrix ------------------------------------------------------------\n",
        "cm = confusion_matrix(all_labels, all_preds, labels=list(range(NUM_CLASSES)))\n",
        "plt.figure(figsize=(12, 10))\n",
        "plt.imshow(cm, cmap='Blues', aspect='auto')\n",
        "plt.title('Confusion Matrix - Test Set')\n",
        "plt.colorbar()\n",
        "step = max(1, NUM_CLASSES // 25)\n",
        "plt.xticks(range(0, NUM_CLASSES, step), [label_map[i] for i in range(0, NUM_CLASSES, step)], rotation=90)\n",
        "plt.yticks(range(0, NUM_CLASSES, step), [label_map[i] for i in range(0, NUM_CLASSES, step)])\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUTPUT_DIR, \"confusion_matrix.png\"), dpi=150)\n",
        "plt.close()\n",
        "\n",
        "# Training Curves -------------------------------------------------------------\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(history['epoch'], history['train_loss'], label='Train Loss')\n",
        "plt.plot(history['epoch'], history['val_loss'], label='Val Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(); plt.grid(True, ls='--', alpha=0.4)\n",
        "plt.tight_layout(); plt.savefig(os.path.join(OUTPUT_DIR, \"loss_curve.png\"), dpi=150)\n",
        "plt.close()\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(history['epoch'], np.array(history['train_acc'])*100, label='Train Acc')\n",
        "plt.plot(history['epoch'], np.array(history['val_acc'])*100, label='Val Acc')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.legend(); plt.grid(True, ls='--', alpha=0.4)\n",
        "plt.tight_layout(); plt.savefig(os.path.join(OUTPUT_DIR, \"accuracy_curve.png\"), dpi=150)\n",
        "plt.close()\n",
        "\n",
        "print(\"\\nDone. Outputs saved in:\", OUTPUT_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q \"/content/drive/MyDrive/Colab Notebooks/test-datasets.zip\" -d /content/testdata"
      ],
      "metadata": {
        "id": "4xtgJ4oycQE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "OEnFP0Uvq6_U",
        "outputId": "c8a2762f-29d0-4cf1-e025-fb5282902f04"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "mount failed",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1408506528.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         )\n\u001b[0;32m--> 277\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: mount failed"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q \"/content/drive/MyDrive/Colab Notebooks/DATASET200.zip\" -d /content/data"
      ],
      "metadata": {
        "id": "B9DIRwBfqrlE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "source_dir = '/content/data/Final DATASET'  # original dataset folder with subfolders 0 to 74\n",
        "target_base = '/content/drive/MyDrive/split_dataset'  # target folder for train/val/test\n",
        "\n",
        "train_ratio = 0.7\n",
        "val_ratio = 0.2\n",
        "test_ratio = 0.1\n",
        "\n",
        "# Create target folders\n",
        "for split in ['train', 'val', 'test']:\n",
        "    os.makedirs(os.path.join(target_base, split), exist_ok=True)\n",
        "    for class_name in os.listdir(source_dir):\n",
        "        os.makedirs(os.path.join(target_base, split, class_name), exist_ok=True)\n",
        "\n",
        "# For each class folder\n",
        "for class_name in tqdm(os.listdir(source_dir), desc=\"Splitting data\"):\n",
        "    class_path = os.path.join(source_dir, class_name)\n",
        "    if not os.path.isdir(class_path):\n",
        "        continue\n",
        "\n",
        "    images = os.listdir(class_path)\n",
        "    random.shuffle(images)\n",
        "\n",
        "    total = len(images)\n",
        "    train_end = int(train_ratio * total)\n",
        "    val_end = train_end + int(val_ratio * total)\n",
        "\n",
        "    train_imgs = images[:train_end]\n",
        "    val_imgs = images[train_end:val_end]\n",
        "    test_imgs = images[val_end:]\n",
        "\n",
        "    for img in train_imgs:\n",
        "        shutil.copy(os.path.join(class_path, img), os.path.join(target_base, 'train', class_name, img))\n",
        "\n",
        "    for img in val_imgs:\n",
        "        shutil.copy(os.path.join(class_path, img), os.path.join(target_base, 'val', class_name, img))\n",
        "\n",
        "    for img in test_imgs:\n",
        "        shutil.copy(os.path.join(class_path, img), os.path.join(target_base, 'test', class_name, img))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g09N_r2vflM-",
        "outputId": "7cf01494-babb-475f-e347-b3ef1e606dc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Splitting data: 100%|██████████| 75/75 [02:05<00:00,  1.67s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!find \"/content/data/Final DATASET\" -mindepth 1 -maxdepth 1 -type d | wc -l\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "na2Joju4fAFl",
        "outputId": "0a2a8c19-825a-46f0-eafc-768bc634aa1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!find \"/content/split_dataset/test\" -mindepth 1 -maxdepth 1 -type d | wc -l\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubYiXVVNjDUx",
        "outputId": "e8c33190-8e77-4f5d-fea6-1ecf228cfa46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from transformers import VisionEncoderDecoderModel, TrOCRProcessor\n",
        "import json\n",
        "import os\n",
        "\n",
        "# =========================\n",
        "# Configuration\n",
        "# =========================\n",
        "OUTPUT_DIR = \"/content/trocr_glyph_out\"  # directory containing best_model.pth and id2label.json\n",
        "MODEL_PATH = os.path.join(OUTPUT_DIR, \"best_model.pth\")\n",
        "BASE_MODEL = \"microsoft/trocr-base-handwritten\"\n",
        "IMG_SIZE = (384, 384)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# =========================\n",
        "# Load label map\n",
        "# =========================\n",
        "with open(os.path.join(OUTPUT_DIR, \"id2label.json\"), \"r\", encoding=\"utf-8\") as f:\n",
        "    label_map = json.load(f)\n",
        "label_map = {int(k): v for k, v in label_map.items()}  # ensure int keys\n",
        "\n",
        "# =========================\n",
        "# Define the Model\n",
        "# =========================\n",
        "class TrOCRClassifier(torch.nn.Module):\n",
        "    def __init__(self, base_model_name: str, num_classes: int):\n",
        "        super().__init__()\n",
        "        base_model = VisionEncoderDecoderModel.from_pretrained(base_model_name)\n",
        "        self.encoder = base_model.encoder\n",
        "        hidden = self.encoder.config.hidden_size\n",
        "        self.classifier = torch.nn.Linear(hidden, num_classes)\n",
        "\n",
        "    def forward(self, pixel_values):\n",
        "        enc_out = self.encoder(pixel_values=pixel_values)\n",
        "        pooled = enc_out.last_hidden_state.mean(dim=1)\n",
        "        return self.classifier(pooled)\n",
        "\n",
        "model = TrOCRClassifier(BASE_MODEL, len(label_map))\n",
        "model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# =========================\n",
        "# Image Preprocessing\n",
        "# =========================\n",
        "processor = TrOCRProcessor.from_pretrained(BASE_MODEL)\n",
        "image_mean = processor.image_processor.image_mean\n",
        "image_std = processor.image_processor.image_std\n",
        "\n",
        "if len(image_mean) == 1:\n",
        "    image_mean = image_mean * 3\n",
        "    image_std = image_std * 3\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(IMG_SIZE),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=image_mean, std=image_std)\n",
        "])\n",
        "\n",
        "# =========================\n",
        "# Prediction Function\n",
        "# =========================\n",
        "def predict_image(image_path: str):\n",
        "    img = Image.open(image_path).convert(\"RGB\")\n",
        "    img_tensor = transform(img).unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        logits = model(img_tensor)\n",
        "        pred_idx = logits.argmax(dim=1).item()\n",
        "        return label_map.get(pred_idx, f\"Unknown ({pred_idx})\")\n",
        "\n",
        "# =========================\n",
        "# Example Usage\n",
        "# =========================\n",
        "if __name__ == \"__main__\":\n",
        "    img_path = \"/content/t1.png\"  # Replace with path to test image\n",
        "    pred_char = predict_image(img_path)\n",
        "    print(f\"Predicted Character: {pred_char}\")\n"
      ],
      "metadata": {
        "id": "AllxImeLq0hF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc8997fa-f658-49e7-e354-50ce6fbb5b2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-handwritten and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Character: ண\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xhUTGkWSLCul"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import pandas as pd\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Configuration\n",
        "DATA_DIR = \"/content/split_dataset/test\"  # <<< UPDATE this to your separate test dataset path\n",
        "MODEL_PATH = \"/content/drive/MyDrive/Tamil NLP Project/TR OCR/best_model.pth\"       # <<< UPDATE this to saved model path\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/eval_output\"          # <<< OUTPUT directory\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "# Label Map (must match original training)\n",
        "label_map = {\n",
        "    0: 'மூ', 1: 'ா', 2: 'ட', 3: 'ம', 4: 'ப', 5: 'ெ', 6: 'ய', 7: 'ல ', 8: 'ன',\n",
        "    9: 'ற', 10: 'க', 11: 'வ', 12: 'ண', 13: 'த', 14: 'ச', 15: 'ங', 16: 'ள',\n",
        "    17: 'தி', 18: 'வி', 19: 'றி', 20: 'லி', 21: 'னி', 22: 'யி', 23: 'ழி',\n",
        "    24: 'பி', 25: 'ரி', 26: 'சி', 27: 'ணி', 28: 'மி', 29: 'கி', 30: 'டு',\n",
        "    31: 'கு', 32: 'ளு', 33: 'லு', 34: 'மு', 35: 'ணு', 36: 'னு', 37: 'ஞ',\n",
        "    38: 'பு', 39: 'று', 40: 'ரு', 41: 'சு', 42: 'து', 43: 'வு', 44: 'யு',\n",
        "    45: 'ழு', 46: 'டி', 47: 'ளி', 48: 'எ', 49: 'ழ', 50: 'கீ', 51: 'றூ',\n",
        "    52: 'மீ', 53: 'வீ', 54: 'நூ', 55: 'றீ', 56: 'தீ', 57: 'கூ', 58: 'தூ',\n",
        "    59: 'சூ', 60: 'யீ', 61: 'லூ', 62: 'உ', 63: 'அ', 64: 'ழீ', 65: 'யூ',\n",
        "    66: 'சீ', 67: 'ணீ', 68: 'ஆ', 69: 'ளு', 70: 'இ', 71: 'ை', 72: 'ர', 73: 'ந', 74: 'ஒ'\n",
        "}\n",
        "NUM_CLASSES = len(label_map)\n",
        "reverse_label_map = {v: k for k, v in label_map.items()}\n",
        "\n",
        "present_classes = np.unique(all_labels)\n",
        "expected_classes = set(range(NUM_CLASSES))\n",
        "missing_classes = expected_classes - set(present_classes)\n",
        "\n",
        "if missing_classes:\n",
        "    print(\" Missing label indices in test set:\", missing_classes)\n",
        "    print(\" Missing characters:\", [label_map[i] for i in missing_classes])\n",
        "else:\n",
        "    print(\" All 75 classes are present in the test set.\")\n",
        "\n",
        "# Preprocessing\n",
        "processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
        "image_mean = processor.image_processor.image_mean\n",
        "image_std = processor.image_processor.image_std\n",
        "\n",
        "eval_transform = transforms.Compose([\n",
        "    transforms.Resize((384, 384)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=image_mean, std=image_std),\n",
        "])\n",
        "\n",
        "\n",
        "# Dataset\n",
        "class GlyphTestDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.samples = []\n",
        "        for label in range(NUM_CLASSES):\n",
        "            label_dir = os.path.join(root_dir, str(label))\n",
        "            if not os.path.isdir(label_dir):\n",
        "                continue\n",
        "            for fname in os.listdir(label_dir):\n",
        "                if fname.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                    self.samples.append((os.path.join(label_dir, fname), label))\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, label = self.samples[idx]\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, label\n",
        "\n",
        "# Model\n",
        "class TrOCRClassifier(torch.nn.Module):\n",
        "    def __init__(self, base_model_name, num_classes):\n",
        "        super().__init__()\n",
        "        base_model = VisionEncoderDecoderModel.from_pretrained(base_model_name)\n",
        "        self.encoder = base_model.encoder\n",
        "        self.classifier = torch.nn.Linear(self.encoder.config.hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, pixel_values):\n",
        "        enc_out = self.encoder(pixel_values=pixel_values)\n",
        "        pooled = enc_out.last_hidden_state.mean(dim=1)\n",
        "        return self.classifier(pooled)\n",
        "\n",
        "# Load model\n",
        "model = TrOCRClassifier(\"microsoft/trocr-base-handwritten\", NUM_CLASSES).to(DEVICE)\n",
        "model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
        "model.eval()\n",
        "\n",
        "# Inference\n",
        "dataset = GlyphTestDataset(DATA_DIR, transform=eval_transform)\n",
        "loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "all_preds, all_labels = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in loader:\n",
        "        imgs = imgs.to(DEVICE)\n",
        "        logits = model(imgs)\n",
        "        preds = logits.argmax(dim=1).cpu().numpy()\n",
        "        all_preds.extend(preds)\n",
        "        all_labels.extend(labels.numpy())\n",
        "\n",
        "report_dict = classification_report(\n",
        "    all_labels,\n",
        "    all_preds,\n",
        "    labels=list(range(NUM_CLASSES)),  # Force all 75 classes\n",
        "    target_names=[label_map[i] for i in range(NUM_CLASSES)],\n",
        "    zero_division=0,\n",
        "    output_dict=True\n",
        ")\n",
        "\n",
        "report_df = pd.DataFrame(report_dict).transpose()\n",
        "report_df.to_csv(os.path.join(OUTPUT_DIR, \"per_class_metrics.csv\"))\n",
        "\n",
        "# Force Pandas to show all rows (disable truncation)\n",
        "pd.set_option('display.max_rows', None)\n",
        "\n",
        "print(\"\\nPer-Class Metrics (saved to CSV):\")\n",
        "print(report_df[['precision', 'recall', 'f1-score', 'support']])\n",
        "\n",
        "# Optional: Confusion matrix\n",
        "cm = confusion_matrix(all_labels, all_preds, labels=list(range(NUM_CLASSES)))\n",
        "plt.figure(figsize=(12, 10))\n",
        "plt.imshow(cm, cmap='Blues', aspect='auto')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.colorbar()\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUTPUT_DIR, \"confusion_matrix.png\"))\n",
        "plt.close()\n",
        "\n",
        "print(f\"\\n Evaluation Complete. Results saved to: {OUTPUT_DIR}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RKuDIovfaZu4",
        "outputId": "e9d9ff64-e6ee-4e52-d749-3d30b9d523f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " All 75 classes are present in the test set.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-handwritten and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Per-Class Metrics (saved to CSV):\n",
            "              precision    recall  f1-score      support\n",
            "மூ             1.000000  1.000000  1.000000    20.000000\n",
            "ா              1.000000  0.950000  0.974359    40.000000\n",
            "ட              1.000000  1.000000  1.000000    20.000000\n",
            "ம              1.000000  0.960000  0.979592    25.000000\n",
            "ப              1.000000  1.000000  1.000000    22.000000\n",
            "ெ              1.000000  1.000000  1.000000    28.000000\n",
            "ய              1.000000  1.000000  1.000000    20.000000\n",
            "ல              1.000000  1.000000  1.000000    21.000000\n",
            "ன              1.000000  1.000000  1.000000    25.000000\n",
            "ற              1.000000  0.950000  0.974359    20.000000\n",
            "க              1.000000  0.960000  0.979592    25.000000\n",
            "வ              0.952381  1.000000  0.975610    20.000000\n",
            "ண              1.000000  1.000000  1.000000    20.000000\n",
            "த              1.000000  1.000000  1.000000    20.000000\n",
            "ச              0.950000  0.950000  0.950000    20.000000\n",
            "ங              1.000000  1.000000  1.000000    20.000000\n",
            "ள              1.000000  1.000000  1.000000    20.000000\n",
            "தி             1.000000  1.000000  1.000000    20.000000\n",
            "வி             1.000000  1.000000  1.000000    20.000000\n",
            "றி             1.000000  1.000000  1.000000    20.000000\n",
            "லி             1.000000  1.000000  1.000000    20.000000\n",
            "னி             1.000000  1.000000  1.000000    20.000000\n",
            "யி             1.000000  1.000000  1.000000    20.000000\n",
            "ழி             1.000000  1.000000  1.000000    20.000000\n",
            "பி             1.000000  1.000000  1.000000    20.000000\n",
            "ரி             1.000000  1.000000  1.000000    20.000000\n",
            "சி             0.952381  1.000000  0.975610    20.000000\n",
            "ணி             1.000000  1.000000  1.000000    20.000000\n",
            "மி             1.000000  1.000000  1.000000    20.000000\n",
            "கி             1.000000  1.000000  1.000000    20.000000\n",
            "டு             1.000000  1.000000  1.000000    20.000000\n",
            "கு             1.000000  1.000000  1.000000    20.000000\n",
            "ளு             0.733333  0.550000  0.628571    20.000000\n",
            "லு             1.000000  1.000000  1.000000    20.000000\n",
            "மு             1.000000  1.000000  1.000000    20.000000\n",
            "ணு             1.000000  0.950000  0.974359    20.000000\n",
            "னு             0.952381  1.000000  0.975610    20.000000\n",
            "ஞ              1.000000  1.000000  1.000000    20.000000\n",
            "பு             1.000000  1.000000  1.000000    20.000000\n",
            "று             1.000000  1.000000  1.000000    20.000000\n",
            "ரு             1.000000  1.000000  1.000000    20.000000\n",
            "சு             1.000000  1.000000  1.000000    20.000000\n",
            "து             1.000000  1.000000  1.000000    20.000000\n",
            "வு             1.000000  1.000000  1.000000    20.000000\n",
            "யு             1.000000  1.000000  1.000000    20.000000\n",
            "ழு             1.000000  1.000000  1.000000    20.000000\n",
            "டி             1.000000  1.000000  1.000000    20.000000\n",
            "ளி             0.952381  1.000000  0.975610    20.000000\n",
            "எ              1.000000  1.000000  1.000000    20.000000\n",
            "ழ              1.000000  1.000000  1.000000    20.000000\n",
            "கீ             1.000000  1.000000  1.000000    20.000000\n",
            "றூ             1.000000  1.000000  1.000000    20.000000\n",
            "மீ             1.000000  1.000000  1.000000    20.000000\n",
            "வீ             1.000000  1.000000  1.000000    20.000000\n",
            "நூ             1.000000  1.000000  1.000000    20.000000\n",
            "றீ             1.000000  1.000000  1.000000    20.000000\n",
            "தீ             1.000000  1.000000  1.000000    20.000000\n",
            "கூ             1.000000  1.000000  1.000000    20.000000\n",
            "தூ             1.000000  1.000000  1.000000    20.000000\n",
            "சூ             1.000000  1.000000  1.000000    20.000000\n",
            "யீ             1.000000  1.000000  1.000000    20.000000\n",
            "லூ             0.952381  1.000000  0.975610    20.000000\n",
            "உ              1.000000  1.000000  1.000000    20.000000\n",
            "அ              1.000000  1.000000  1.000000    20.000000\n",
            "ழீ             1.000000  1.000000  1.000000    20.000000\n",
            "யூ             1.000000  1.000000  1.000000    20.000000\n",
            "சீ             1.000000  1.000000  1.000000    20.000000\n",
            "ணீ             1.000000  0.950000  0.974359    20.000000\n",
            "ஆ              1.000000  1.000000  1.000000    20.000000\n",
            "இ              1.000000  1.000000  1.000000    20.000000\n",
            "ை              1.000000  1.000000  1.000000    20.000000\n",
            "ர              0.909091  1.000000  0.952381    20.000000\n",
            "ந              0.952381  1.000000  0.975610    20.000000\n",
            "ஒ              1.000000  1.000000  1.000000    20.000000\n",
            "accuracy       0.986417  0.986417  0.986417     0.986417\n",
            "macro avg      0.986312  0.986933  0.986247  1546.000000\n",
            "weighted avg   0.986719  0.986417  0.986192  1546.000000\n",
            "\n",
            " Evaluation Complete. Results saved to: /content/drive/MyDrive/eval_output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TROCR"
      ],
      "metadata": {
        "id": "3FI1koJPOPth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install evaluate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mrggw1_XO3CH",
        "outputId": "0cc8e177-818b-4805-8b74-7c282f2cbe37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.5-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.14.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.0.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.33.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.11.15)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.6.15)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
            "Downloading evaluate-0.4.5-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m434.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: evaluate\n",
            "Successfully installed evaluate-0.4.5\n"
          ]
        }
      ]
    }
  ]
}